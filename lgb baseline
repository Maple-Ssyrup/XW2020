#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun 24 21:08:52 2020

@author: sallyjiang
"""
#Part 1 EDA + Scenario Classification 

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

## Step A: EDA 
data_path = "~/Documents/2020XW/"
train = pd.read_csv(data_path+'sensor_train.csv')
test = pd.read_csv(data_path+'sensor_test.csv')
#train = pd.read_csv('sensor_train.csv')
#test = pd.read_csv('sensor_test.csv')

label = 'behavior_id'
print('train and test : ({},{})'.format(train.shape, test.shape))

# 1. Check missing value & duplicates
print('Number of duplicates in train: {}'.format(sum(train.duplicated())))
print('Number of duplicates in test : {}'.format(sum(test.duplicated())))

print('We have {} NaN/Null values in train'.format(train.isnull().values.sum()))
print('We have {} NaN/Null values in test'.format(test.isnull().values.sum()))

# 2. Decode 'behavior_id':
# 2.1 
def trans(label):
        mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', 
        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', 
        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', 
        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', 
        16: 'C_2', 17: 'C_5', 18: 'C_6'}
    # 将行为ID转为编码
        code = [mapping[i] for i in label]
        return code

def add_code(df):
    df['code'] = pd.Series(trans(df[label]))
    return df  

train = add_code(train)

# 2.2 Add hand phone infomation (dummy) 
def add_handphone(df):
    handphone = []
    for a in df['code']:
        if a.split("_")[0] == 'D':
            handphone.append(1)
        else:
            handphone.append(0)
    if len(df[label]) == len(handphone):
        df['handphone'] = pd.Series(handphone)
        return df
    else:
        print('ERROR')
        
train = add_handphone(train)

# 2.3 separate 'scenario' and 'motion' from code
train['scenario'] = train.apply(lambda row: row.code.split('_')[0], axis = 1)
train['motion'] = train.apply(lambda row: row.code.split('_')[1], axis = 1)

for col in train.columns: 
    print(col)
train.head(10)

# 3. Visulization 
sns.set_style('whitegrid')
plt.rcParams['font.family'] = 'Dejavu Sans'

# 3.1 Handphone
plt.figure(figsize=(12,6))
plt.title('Data provided by each user', fontsize=20)
sns.countplot(x=label, hue='handphone', data = train) # label!
plt.show()

# 3.2 Motion
plt.figure(figsize=(12,6))
plt.title('Data provided by each user', fontsize=20)
sns.countplot(x='motion', hue='motion', data = train)
plt.show()

# 4. Transformation:

# 4.1 each fragment_id get one record with basic stats 
# trainingset: 
data = train
df = train.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id', 'behavior_id','motion','handphone','scenario']]

data['acc'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5
data['accg'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5


for f in tqdm([f for f in data.columns if 'acc' in f]):
    for stat in ['min', 'max', 'mean', 'median', 'std', 'skew']:
        df[f+'_'+stat] = data.groupby('fragment_id')[f].agg(stat).values
        
df.head(10)
#testingset
df_test = test.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id']]

data = test
data['acc'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5
data['accg'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5

from tqdm import tqdm
for f in tqdm([f for f in data.columns if 'acc' in f]):
    for stat in ['min', 'max', 'mean', 'median', 'std', 'skew']:
        df_test[f+'_'+stat] = data.groupby('fragment_id')[f].agg(stat).values
 
df_test.info()

# 4.2 acc/accg mean (by 'motion')
#### From the six-motion perspective, acc is a little bit better than accg

sns.set_palette("Set1", desat=0.80)
facetgrid = sns.FacetGrid(df, hue='motion', height=6,aspect=2)
facetgrid.map(sns.distplot,'acc_mean', hist=False)\
    .add_legend()
plt.show()

sns.set_palette("Set1", desat=0.80)
facetgrid = sns.FacetGrid(df, hue='motion', height=6,aspect=2)
facetgrid.map(sns.distplot,'accg_mean', hist=False)\
    .add_legend()
plt.show()


# 4.3 acc/accg std (by 'motion')
sns.set_palette("Set1", desat=0.80)
facetgrid = sns.FacetGrid(df, hue='motion', size=6,aspect=2)
facetgrid.map(sns.distplot,'acc_std', hist=False)\
    .add_legend()
plt.show()


sns.set_palette("Set1", desat=0.80)
facetgrid = sns.FacetGrid(df, hue='motion', size=6,aspect=2)
facetgrid.map(sns.distplot,'accg_std', hist=False)\
    .add_legend()
plt.show()

# 4.4 Motion
## 1 out numbers other behaviors, 6 is the next 

plt.figure(figsize=(12,6))
plt.title('Data provided by each user', fontsize=20)
sns.countplot(x='motion', hue='motion', data = df)
plt.show()

# 4.5 Scenario  
###### We can see that behavior 1,6,12 (which is A_1, B_1, C_1) out numbers the other type of behaviors
plt.figure(figsize=(16,8))
plt.title('Data provided by each user', fontsize=20)
sns.countplot(x=label, hue='scenario', data = df)
plt.show()


# 4.6 acc/accg mean (by 'scenario')
##### From the scenario perspective, acc is always better than accg !
sns.set_palette("Set1", desat=0.80)
facetgrid = sns.FacetGrid(df, hue='scenario', size=6,aspect=2)
facetgrid.map(sns.distplot,'acc_mean', hist=False)\
    .add_legend()
plt.show()

sns.set_palette("Set1", desat=0.80)
facetgrid = sns.FacetGrid(df, hue='scenario', size=6,aspect=2)
facetgrid.map(sns.distplot,'accg_mean', hist=False)\
    .add_legend()
plt.show()

# 4.7 acc/accg mean (by 'scenario')
sns.set_palette("Set1", desat=0.80)
facetgrid = sns.FacetGrid(df, hue='scenario', size=6,aspect=2)
facetgrid.map(sns.distplot,'acc_std', hist=False)\
    .add_legend()
plt.show()

sns.set_palette("Set1", desat=0.80)
facetgrid = sns.FacetGrid(df, hue='scenario', size=6,aspect=2)
facetgrid.map(sns.distplot,'accg_std', hist=False)\
    .add_legend()
plt.show()

# Step B. Modelling: use acc is key feature to identify D,C and (A+B) 

# 1. Combine A,B as 'AB'
def scenario_3class(scenario):
    if scenario in ['A','B']:
        return 'AB'
    else:
        return scenario
    
df['scenario_3class'] = df.apply(lambda row: scenario_3class(row.scenario), axis = 1)

df.head(10)

# 2. Split Dataset 
del_feature = ['fragment_id','behavior_id','motion','handphone','scenario','scenario_3class']
features = [i for i in df.columns if i not in del_feature]

X = df[features]
Y = df['scenario_3class']
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.15)

####public test set
X_test_public = df_test.drop(['fragment_id'], axis=1) 

label=['AB','C','D']
print('X_train and y_train : ({},{})'.format(X_train.shape, Y_train.shape))
print('X_test  and Y_test  : ({},{})'.format(X_test.shape, Y_test.shape))


# 3. Some functions  
# 3.1 Confusion Matrix
import itertools
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
plt.rcParams["font.family"] = 'DejaVu Sans'

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    
    
# 3.2 Generic func
from datetime import datetime
def perform_model(model, X_train, y_train, X_test, y_test, class_labels, cm_normalize=True, \
                 print_cm=True, cm_cmap=plt.cm.Greens):
    
    
    # to store results at various phases
    results = dict()
    
    # time at which model starts training 
    train_start_time = datetime.now()
    print('training the model..')
    model.fit(X_train, y_train)
    print('Done \n \n')
    train_end_time = datetime.now()
    results['training_time'] =  train_end_time - train_start_time
    print('training_time(HH:MM:SS.ms) - {}\n\n'.format(results['training_time']))
    
    
    # predict test data
    print('Predicting test data')
    test_start_time = datetime.now()
    y_pred = model.predict(X_test)
    test_end_time = datetime.now()
    print('Done \n \n')
    results['testing_time'] = test_end_time - test_start_time
    print('testing time(HH:MM:SS:ms) - {}\n\n'.format(results['testing_time']))
    results['predicted'] = y_pred
   

    # calculate overall accuracty of the model
    accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)
    # store accuracy in results
    results['accuracy'] = accuracy
    print('---------------------')
    print('|      Accuracy      |')
    print('---------------------')
    print('\n    {}\n\n'.format(accuracy))
    
    
    # confusion matrix
    cm = metrics.confusion_matrix(y_test, y_pred)
    results['confusion_matrix'] = cm
    if print_cm: 
        print('--------------------')
        print('| Confusion Matrix |')
        print('--------------------')
        print('\n {}'.format(cm))
        
    # plot confusin matrix
    plt.figure(figsize=(8,8))
    plt.grid(b=False)
    plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='Normalized confusion matrix', cmap = cm_cmap)
    plt.show()
    
    # get classification report
    print('-------------------------')
    print('| Classifiction Report |')
    print('-------------------------')
    classification_report = metrics.classification_report(y_test, y_pred)
    # store report in results
    results['classification_report'] = classification_report
    print(classification_report)
    
    # add the trained  model to the results
    results['model'] = model
    
    return results
   
    
# 3.3 Grid Search
def print_grid_search_attributes(model):
    # Estimator that gave highest score among all the estimators formed in GridSearch
    print('--------------------------')
    print('|      Best Estimator     |')
    print('--------------------------')
    print('\n\t{}\n'.format(model.best_estimator_))


    # parameters that gave best results while performing grid search
    print('--------------------------')
    print('|     Best parameters     |')
    print('--------------------------')
    print('\tParameters of best estimator : \n\n\t{}\n'.format(model.best_params_))


    #  number of cross validation splits
    print('---------------------------------')
    print('|   No of CrossValidation sets   |')
    print('--------------------------------')
    print('\n\tTotal numbre of cross validation sets: {}\n'.format(model.n_splits_))


    # Average cross validated score of the best estimator, from the Grid Search 
    print('--------------------------')
    print('|        Best Score       |')
    print('--------------------------')
    print('\n\tAverage Cross Validate scores of best estimator : \n\n\t{}\n'.format(model.best_score_))
    
    
# 4. Modelling
# 4.1 GBDT  
labels=['AB','C','D']

del_feature = ['fragment_id','behavior_id','motion','handphone','scenario','scenario_3class']
features = [i for i in df.columns if i not in del_feature]

X = df[features]
Y = df['scenario_3class']
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.15)

####public test set
X_test_public = df_test.drop(['fragment_id'], axis=1) 

print('X_train and y_train : ({},{})'.format(X_train.shape, Y_train.shape))
print('X_test  and Y_test  : ({},{})'.format(X_test.shape, Y_test.shape))

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import linear_model
from sklearn import metrics

param_grid = {'max_depth': np.arange(5,8,1), \
             'n_estimators':np.arange(130,170,10)}
gbdt = GradientBoostingClassifier()
gbdt_grid = GridSearchCV(gbdt, param_grid=param_grid, cv=3, verbose = 1, n_jobs=-1)
gbdt_grid_results = perform_model(gbdt_grid, X_train, Y_train, X_test, Y_test, class_labels=labels)
print_grid_search_attributes(gbdt_grid_results['model'])

# 4.2 Random Forest
from sklearn.ensemble import RandomForestClassifier
params = {'n_estimators': np.arange(10,201,20), 'max_depth':np.arange(3,15,2)}
rfc = RandomForestClassifier()
rfc_grid = GridSearchCV(rfc, param_grid=params, cv=3, verbose=1, n_jobs=-1)
rfc_grid_results = perform_model(rfc_grid, X_train, Y_train, X_test, Y_test, class_labels=labels)

print_grid_search_attributes(rfc_grid_results['model'])

# 4.3 Prediction: 
## Thus use GBM as the AB,C, D classifier

sub = pd.read_csv('提交结果示例.csv')
print('train and sub and df_test : ({},{},{})'.format(test.shape,sub.shape,df_test.shape))

## predict labels
X_test_public = df_test.drop(['fragment_id'], axis=1) 
pred_scenario_3class = gbdt_grid.predict(X_test_public)
sub['pred_scenario_3class'] = pred_scenario_3class
df_test.head()

## merge test and sub
df_test_3class=pd.merge(df_test,sub,how='inner')
df_test_3class.head(3)


# Step C. Modelling: identify A, B from mixed 'AB'

from sklearn.model_selection import train_test_split
# 1. data
labels = ['A','B']  
df_ab = df[df['scenario_3class'] =='AB']

del_feature = ['fragment_id','behavior_id','motion','handphone','scenario','scenario_3class']

features = [i for i in df.columns if i not in del_feature]

X = df_ab[features]
Y = df_ab['scenario']
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.15)

####public test set
X_test_public = df_test_3class[df_test_3class['pred_scenario_3class'] =='AB'].drop(['fragment_id'], axis=1) 

X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.15)

#2. Modelling 
# 2.1 Logistic Regression
parameters = {'C':[0.01, 0.1, 1, 10, 20, 30], 'penalty':['l2','l1']}
log_reg = linear_model.LogisticRegression()
log_reg_grid = GridSearchCV(log_reg, param_grid=parameters, cv=3, verbose=1, n_jobs=-1)
log_reg_grid_results =  perform_model(log_reg_grid, X_train, Y_train, X_test, Y_test, class_labels=labels)

# 2.2 Linear SVM
from sklearn.svm import LinearSVC
parameters = {'C':[0.125, 0.5, 1, 2, 8, 16]}
lr_svc = LinearSVC(tol=0.00005)
lr_svc_grid = GridSearchCV(lr_svc, param_grid=parameters, cv=3, verbose=1, n_jobs=-1)
lr_svc_grid_results = perform_model(lr_svc_grid, X_train, Y_train, X_test, Y_test, class_labels=labels)
print_grid_search_attributes(lr_svc_grid_results['model'])

#2.3 Non-linear SVM
from sklearn.svm import SVC
parameters = {'C':[2,8,16],\
              'gamma': [ 0.0078125, 0.125, 2]}
rbf_svm = SVC(kernel='rbf')
rbf_svm_grid = GridSearchCV(rbf_svm,param_grid=parameters, cv=3, verbose=1, n_jobs=-1)

rbf_svm_grid_results = perform_model(rbf_svm_grid, X_train, Y_train, X_test, Y_test, class_labels=labels)

#2.4 Random Forest
from sklearn.ensemble import RandomForestClassifier
params = {'n_estimators': np.arange(10,201,20), 'max_depth':np.arange(3,15,2)}
rfc = RandomForestClassifier()
rfc_grid = GridSearchCV(rfc, param_grid=params, cv=3, verbose=1, n_jobs=-1)
rfc_grid_results = perform_model(rfc_grid, X_train, Y_train, X_test, Y_test, class_labels=labels)

print_grid_search_attributes(rfc_grid_results['model'])

#2.5 Grandient Boosting
from sklearn.ensemble import GradientBoostingClassifier
param_grid = {'max_depth': np.arange(5,8,1), \
             'n_estimators':np.arange(130,170,10)}
gbdt = GradientBoostingClassifier()
gbdt_grid = GridSearchCV(gbdt, param_grid=param_grid, cv=3, verbose=1,n_jobs=-1)
gbdt_grid_results = perform_model(gbdt_grid, X_train, Y_train, X_test, Y_test, class_labels=labels)

print_grid_search_attributes(gbdt_grid_results['model'])

# 2.6 Prediction: 
## Thus choose Grandient Boosting model for A/B Bineary Classification 
df_test_3class.info()
####public test set
X_test_public = df_test_3class[df_test_3class['pred_scenario_3class'] =='AB'].drop(['fragment_id','behavior_id','pred_scenario_3class'], axis=1)
pred_scenario = gbdt_grid.predict(X_test_public)

a = pd.DataFrame(pred_scenario)
f_id = df_test_3class[df_test_3class['pred_scenario_3class'] =='AB']['fragment_id']
a['fragment_id'] = np.array(f_id)
a['pred_scenario']= pred_scenario

a = a.drop(a.columns[0], axis=1)
a.head(10)

sub.head(10)
sub_a = pd.merge(sub,a,how='left')
sub_a['Scenario'] = np.where(sub_a['pred_scenario'].isnull(),sub_a['pred_scenario_3class'], sub_a['pred_scenario'])
sub_a = sub_a.drop(['pred_scenario_3class','pred_scenario'], axis=1)
sub_a.head(10)






# Part 2 LSTM 
# Step D: For scenario A, B, C build LSTM model

import pandas as pd
import numpy as np

def acc_combo(y, y_pred):
    # 数值ID与行为编码的对应关系
    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', 
        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', 
        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', 
        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', 
        16: 'C_2', 17: 'C_5', 18: 'C_6'}
    # 将行为ID转为编码
    code_y, code_y_pred = mapping[y], mapping[y_pred]
    if code_y == code_y_pred: #编码完全相同得分1.0
        return 1.0
    elif code_y.split("_")[0] == code_y_pred.split("_")[0]: #编码仅字母部分相同得分1.0/7
        return 1.0/7
    elif code_y.split("_")[1] == code_y_pred.split("_")[1]: #编码仅数字部分相同得分1.0/3
        return 1.0/3
    else:
        return 0.0

# 1. Data
# 1.1  
data_path = "~/Documents/2020XW/"
train = pd.read_csv(data_path+'sensor_train.csv')
test = pd.read_csv(data_path+'sensor_test.csv')
#train = pd.read_csv('sensor_train.csv')
#test = pd.read_csv('sensor_test.csv')

print('train and test : ({},{})'.format(train.shape, test.shape))

label = 'behavior_id'

def trans(label):
        mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', 
        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', 
        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', 
        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', 
        16: 'C_2', 17: 'C_5', 18: 'C_6'}
    # 将行为ID转为编码
        code = [mapping[i] for i in label]
        return code

def add_code(df):
    df['code'] = pd.Series(trans(df[label]))
    return df  

train = add_code(train)

train['scenario'] = train.apply(lambda row: row.code.split('_')[0], axis = 1)
train['motion'] = train.apply(lambda row: row.code.split('_')[1], axis = 1)

def add_handphone(df):
    handphone = []
    for a in df['code']:
        if a.split("_")[0] == 'D':
            handphone.append(1)
        else:
            handphone.append(0)
    if len(df[label]) == len(handphone):
        df['handphone'] = pd.Series(handphone)
        return df
    else:
        print('ERROR')
        
        
train = add_handphone(train)
train.head()



data = train
df = train.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id', 
                                                                           'behavior_id','motion','handphone','scenario']]
data['acc_'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5
#data['accg'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5
from tqdm import tqdm
for f in tqdm([f for f in data.columns if 'acc_' in f]):
    for stat in ['min', 'max', 'mean', 'median', 'std', 'skew']:
        df[f+'_'+stat] = data.groupby('fragment_id')[f].agg(stat).values
        
        
        
data = test
df_test = test.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id']]
data['acc_'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5
data['accg'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5
from tqdm import tqdm
for f in tqdm([f for f in data.columns if 'acc' in f]):
    for stat in ['min', 'max', 'mean', 'median', 'std', 'skew']:
        df_test[f+'_'+stat] = data.groupby('fragment_id')[f].agg(stat).values
            
        
        
# 1.2 Get ABC scenario        
train_abc = train[~(train.behavior_id == 4)].reset_index(drop=True)
train_abc.groupby(by = 'scenario',sort = False).motion.nunique()
train_abc = train_abc.drop(['code','scenario','behavior_id','handphone'],axis=1)
train_abc['ori_index'] = train_abc.index
train_abc['n_frag'] = train_abc.groupby(by='fragment_id', sort=False)['time_point'].transform('nunique').astype(int)
train_abc.head()
train_abc.shape
        #(Public test):
test['ori_index'] = test.index
test['n_frag'] = test.groupby(by='fragment_id', sort=False)['time_point'].transform('nunique').astype(int)
test.head()
test.shape


#2. Get the balanced dataset(same time_point length): 
import random as rd
import dask
from dask.diagnostics import ProgressBar

base = train_abc['n_frag'].max()
        #(Public test):
base_test = test['n_frag'].max()    

def add_time(grp): # df, one fragment_id
    grp = grp.reset_index(drop=True).sort_values(by=['fragment_id','time_point'])
    count = base - len(grp['time_point']) 
    add_index = [rd.randint(1, len(grp['time_point'])-1 ) for _ in range(count)]
    
    for i in add_index:
        line = pd.DataFrame(grp.loc[i:i+1].mean(axis=0))
        line = line.transpose()
        grp = pd.concat([grp.loc[0:i], line, grp.loc[i+1:1000]]).reset_index(drop=True)
        
        
        
def apply_to_grp(df):

    frag_id = df.fragment_id.unique()

    count = 0

    for id in frag_id:
    
        intact = df[df['fragment_id']==id]
        added = add_time(intact)
        added['grp_index'] = added.index
        added.fragment_id = id     ############## WTF????????????
        
        if count == 0:
            
            result = added
            
            count = 1
        else:
            result = result.append(added)
            
    return result.reset_index(drop=True).sort_values(by=['fragment_id','time_point'])

    return grp




train_abc_balanced = apply_to_grp(train_abc)
train_abc_balanced.head()
train_abc_balanced.groupby(['fragment_id']).size().min()
train_abc_balanced['timestamp'] = train_abc_balanced['grp_index']
train_abc_balanced['n_frag'] = train_abc_balanced.groupby(by='fragment_id', sort=False)['timestamp'].transform('nunique').astype(int)
train_abc_balanced['n_frag'].describe()

        #(Public test):
test_balanced = apply_to_grp(test)
test_balanced.head()
test_balanced.groupby(['fragment_id']).size().min()
test_balanced['timestamp'] = test_balanced['grp_index']
test_balanced['n_frag'] = test_balanced.groupby(by='fragment_id', sort=False)['timestamp'].transform('nunique').astype(int)
test_balanced['n_frag'].describe()


#3.Modelling  LSTM 
#3.1 Input 
train_abc_balanced['timestamp'] = train_abc_balanced['grp_index']

def convert(df):

    frag_id = df.fragment_id.unique()
    y_train = df.groupby('fragment_id').first()['motion'].values
    
    #convert y
    def convert_y(df): # one column pd df
        return pd.get_dummies(y_train, prefix='dum')
    
    y_train = convert_y(train_abc_balanced.groupby('fragment_id').first()['motion'])
    Y_train = y_train.values
    
    # convert x
    count1 = 0
    for id in frag_id:
        tmp = train_abc_balanced[train_abc_balanced['fragment_id'] == id]
        tmp = tmp.drop(columns = ['fragment_id','time_point','motion','n_frag',
               'ori_index', 'grp_index'])
     
        # one fragment
        count2 = 0
        for i in range(len(tmp['timestamp'])):
            row = tmp[tmp['timestamp']==i].iloc[:,0:7].values
            if count2 == 0:
                result2 = row
                count2 = 1
            else:
                result2 = np.concatenate((result2, row))
        
        if count1 == 0:            
            result1 = result2
            count1 = 1
        elif count1 == 1:
            result1 = np.concatenate(([result1], [result2]))
            count1 = 2
        else:
            result1 = np.concatenate((result1, [result2]))
        
    X_train = result1
        
    return X_train, Y_train






def convert_test(df):

    frag_id = df.fragment_id.unique()
    
    # convert x
    count1 = 0
    for id in frag_id:
        tmp = test_balanced[test_balanced['fragment_id'] == id]
        tmp = tmp.drop(columns = ['fragment_id','time_point','n_frag',
               'ori_index', 'grp_index'])
     
        # one fragment
        count2 = 0
        for i in range(len(tmp['timestamp'])):
            row = tmp[tmp['timestamp']==i].iloc[:,0:7].values
            if count2 == 0:
                result2 = row
                count2 = 1
            else:
                result2 = np.concatenate((result2, row))
        
        if count1 == 0:            
            result1 = result2
            count1 = 1
        elif count1 == 1:
            result1 = np.concatenate(([result1], [result2]))
            count1 = 2
        else:
            result1 = np.concatenate((result1, [result2]))
        
    X_test = result1
        
    return X_test, frag_id





#Get training features and lables for LSTM
X_train, Y_train = convert(train_abc_balanced)
        
timesteps = len(X_train[0])
input_dim = len(X_train[0][0])
n_classes = len(Y_train[0])
print(timesteps)
print(input_dim)
print(n_classes)
print(len(X_train))
print(len(Y_train))


from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_train,Y_train,test_size=0.15)


#3.2 Initializing parameters
epochs = 30
batch_size = 32
n_hidden = 64
pv = 0.25


from keras.models import Sequential
from keras.layers import LSTM
from keras.layers.core import Dense, Dropout
from keras.layers.normalization import BatchNormalization
# Initiliazing the sequential model
model = Sequential()
# Configuring the parameters
model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))
model.add(BatchNormalization())
# Adding a dropout layer
model.add(Dropout(pv))
# Adding a dense output layer with sigmoid activation
model.add(Dense(n_classes, activation='sigmoid'))
model.summary()



# Compiling the model
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])



# Training the model
model.fit(X_train,
          Y_train,
          batch_size=batch_size,
          validation_data=(X_test, Y_test),
          epochs=epochs)


# 3.3 Predictions: 
###Get Public_test input 

X_test, test_frag_id = convert_test(test_balanced)

print(len(X_test[0]))
print(len(X_test[0][0]))
print(len(X_test))
print(len(test_frag_id))

pred_motion = model.predict(X_test)
        
        
b = pd.DataFrame(pred_motion)
b['fragment_id'] = np.array(test_frag_id)
b['pred_motion']= pred_motion
b = b.drop(b.columns[0], axis=1)
b.head(10)


sub_a.head(10)
sub_b = pd.merge(sub_a,b,how='left')
#####
sub_b['Motion'] = np.where(sub_b['Scenario']=='D','4', sub_a['pred_motion'])
sub_b = sub_b.drop(['behavior_id','pred_motion'], axis=1)
sub_b['behavior_code'] = sub_b['Scenario'] + '_' + sub_a['Motion']
sub_b.head(10)


label = 'behavior_code'
def trans(label):
        mapping = {'A_0':0 , 'A_1':1 , 'A_2':2 , 'A_3':3, 
                   'D_4':4 , 'A_5':5 , 'B_1':6 , 'B_5':7, 
                   'B_2':8 , 'B_3':9 , 'B_0':10 , 'A_6':11, 
                   'C_1':12, 'C_3':13, 'C_0':14 , 'B_6':15, 
                   'C_2':16, 'C_5':17, 'C_6':18}
    # 将行为编码转为id
        id = [mapping[i] for i in label]
        return id

def add_id(df):
    df['behavior_id'] = pd.Series(trans(df[label]))
    return df  

sub_b = add_id(sub_b)
sub_final = sub_b.drop(['behavior_code','pred_motion','Scenario', 'Motion'], axis=1)
sub_final.to_csv("./sub_final0704.csv") 




                     
