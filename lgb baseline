#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun 24 21:08:52 2020

@author: sallyjiang
"""

import numpy as np
import pandas as pd

from sklearn.model_selection import StratifiedKFold
pip install lightgbm
import lightgbm as lgb
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from tqdm import tqdm
import matplotlib.pyplot as plt
from scipy.stats import entropy
import gc
import os


data_path = "~/Documents/2020XW/"
data_train = pd.read_csv(data_path+'sensor_train.csv')
data_test = pd.read_csv(data_path+'sensor_test.csv')
data_test['fragment_id'] += 10000
label = 'behavior_id'
data = pd.concat([data_train, data_test], sort=False)
print("shape: ",data.shape, data_train.shape, data_test.shape)


data.isnull().sum()
#remove duplicates
df = data.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id', 'behavior_id']]
df.head(20)

data['acc'] = (data['acc_x'] ** 2 + data['acc_y'] ** 2 + data['acc_z'] ** 2) ** 0.5
data['accg'] = (data['acc_xg'] ** 2 + data['acc_yg'] ** 2 + data['acc_zg'] ** 2) ** 0.5

for f in tqdm([f for f in data.columns if 'acc' in f]):
    for stat in ['min', 'max', 'mean', 'median', 'std', 'skew']:
        df[f+'_'+stat] = data.groupby('fragment_id')[f].agg(stat).values

train_df = df[df[label].isna()==False].reset_index(drop=True)
test_df = df[df[label].isna()==True].reset_index(drop=True)
print("df_shape:", df.shape, train_df.shape,test_df.shape)
train_df.head(20)
df.isnull().sum()

drop_feat = []
###这句没看懂
used_feat = [f for f in train_df.columns if f not in (['fragment_id', label] + drop_feat)]
print("used & drop feat:", len(used_feat),len(drop_feat))
print("used_feat", used_feat)

train_x = train_df[used_feat]
train_y = train_df[label]
test_x = test_df[used_feat]
test_x.shape,train_x.shape,train_y.shape


def score_err(clf,X_test,y_test):   #######define error rate
    pred_y = clf.predict(X_test)
    return 1-np.mean(pred_y == y_test
                     
                     
def acc_combo(y, y_pred):           ######define acc_combo 
    # 数值ID与行为编码的对应关系
    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', 
        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', 
        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', 
        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', 
        16: 'C_2', 17: 'C_5', 18: 'C_6'}
    # 将行为ID转为编码
    code_y, code_y_pred = mapping[y], mapping[y_pred]
    if code_y == code_y_pred: #编码完全相同得分1.0
        return 1.0
    elif code_y.split("_")[0] == code_y_pred.split("_")[0]: #编码仅字母部分相同得分1.0/7
        return 1.0/7
    elif code_y.split("_")[1] == code_y_pred.split("_")[1]: #编码仅数字部分相同得分1.0/3
        return 1.0/3
    else:
        return 0.0

####用zip匹配计算####                    
score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(train_y, oof_y)) / oof_y.shape[0]
round(score, 5)
                     
####basic classifiers 
acc_svm=[]

for i in range(3):
    X_svm = train_df.values
    
    svm_train_len = int(len(train_df)*0.7)
    
    np.random.seed(i*16 + 97)
    np.random.shuffle(X_svm)
    train_svm = X_svm[:svm_train_len]
    val_svm = X_svm[svm_train_len:]
    
    np.random.seed(i*11 + 79)
    np.random.shuffle(train_svm)
    feature_train_svm = train_svm[:,2:]
    label_train_svm = train_svm[:,1]
    feature_val_svm = val_svm[:,2:]
    label_val_svm = val_svm[:,1]
    
    
    only_svm = LinearSVC(loss = 'hinge', C = 500, class_weight = 'balanced')
    only_svm.fit(feature_train_svm,label_train_svm)
    print('第{}次SVM的误差为{}:'.format(i+1,score_err(only_svm,feature_val_svm,label_val_svm)))
    print('第{}次SVM的分数为{}:'.format(i+1,acc_combo(only_svm,feature_val_svm,label_val_svm)))

    acc_svm.append(acc_combo(only_svm,feature_val_svm,label_val_svm))

    
    
####lgb baseline
scores = []
imp = pd.DataFrame()
imp['feat'] = used_feat

params = {
    'learning_rate': 0.1,
    'metric': 'multi_error',
    'objective': 'multiclass',
    'num_class': 19,
    'feature_fraction': 0.80,
    'bagging_fraction': 0.75,
    'bagging_freq': 2,
    'n_jobs': 4,
    'seed': 2020,
    'max_depth': 10,
    'num_leaves': 64,
    'lambda_l1': 0.5,
    'lambda_l2': 0.5,
}

oof_train = np.zeros((len(train_x), 19))
preds = np.zeros((len(test_x), 19))
folds = 5
seeds = [44 , 2020, 527, 1527]
for seed in seeds:
    kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)
    for fold, (trn_idx, val_idx) in enumerate(kfold.split(train_x, train_y)):
        x_trn, y_trn, x_val, y_val = train_x.iloc[trn_idx], train_y.iloc[trn_idx], train_x.iloc[val_idx], train_y.iloc[val_idx]
        train_set = lgb.Dataset(x_trn, y_trn)
        val_set = lgb.Dataset(x_val, y_val)

        model = lgb.train(params, train_set, num_boost_round=500000,
                          valid_sets=(train_set, val_set), early_stopping_rounds=50,
                          verbose_eval=20)
        oof_train[val_idx] += model.predict(x_val) / len(seeds)
        preds += model.predict(test_x) / folds / len(seeds)
        scores.append(model.best_score['valid_1']['multi_error'])
        imp['gain' + str(fold + 1)] = model.feature_importance(importance_type='gain')
        imp['split' + str(fold + 1)] = model.feature_importance(importance_type='split')
        del x_trn, y_trn, x_val, y_val, model, train_set, val_set
        gc.collect()
                     
                     
                     
